"""
Report Generator
Generates HTML and PDF reports from training results.
"""

import json
import os
import base64
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import tempfile
import subprocess

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.io as pio
from jinja2 import Environment, FileSystemLoader
import pickle

logger = logging.getLogger(__name__)

class ReportGenerator:
    """Generate comprehensive training reports."""
    
    def __init__(self, template_dir: str = "reports/templates"):
        """
        Initialize report generator.
        
        Args:
            template_dir: Directory containing report templates
        """
        self.template_dir = template_dir
        self.env = Environment(loader=FileSystemLoader(template_dir))
        
        # Load branding configuration
        self.config = self._load_report_config()
    
    def _load_report_config(self) -> Dict[str, Any]:
        """Load report configuration."""
        config_path = "report_config.json"
        
        if os.path.exists(config_path):
            with open(config_path) as f:
                return json.load(f)
        else:
            # Default configuration
            return {
                "brand_name": "ML Training Platform",
                "primary_color": "#FF6B6B",
                "secondary_color": "#4ECDC4",
                "logo_url": "",
                "footer_text": "Generated by ML Training Platform"
            }
    
    def generate_single_run_report(self, run_id: str) -> Optional[str]:
        """
        Generate HTML report for a single training run.
        
        Args:
            run_id: ID of the training run
            
        Returns:
            HTML report string or None if failed
        """
        try:
            # Load run data
            run_data = self._load_run_data(run_id)
            if not run_data:
                logger.error(f"Failed to load data for run {run_id}")
                return None
            
            # Generate report sections
            context = {
                'title': f'Training Report - {run_data["config"].get("run_label", run_id)}',
                'subtitle': f'Run ID: {run_id}',
                'is_comparison': False,
                'primary_color': self.config['primary_color'],
                'secondary_color': self.config['secondary_color'],
                'logo_url': self.config.get('logo_url', ''),
                'footer_text': self.config['footer_text'],
                'generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'git_commit': run_data['config'].get('git_commit', 'unknown'),
                
                # Page numbers (for PDF)
                'executive_summary_page': '3',
                'dataset_analysis_page': '4',
                'architecture_page': '5',
                'training_page': '6',
                'evaluation_page': '7',
                'error_analysis_page': '8',
                'conclusions_page': '9',
                
                # Content sections
                'executive_summary': self._generate_executive_summary(run_data),
                'dataset_analysis': self._generate_dataset_analysis(run_data),
                'model_architecture': self._generate_model_architecture(run_data),
                'training_results': self._generate_training_results(run_data),
                'evaluation_metrics': self._generate_evaluation_metrics(run_data),
                'error_analysis': self._generate_error_analysis(run_data),
                'conclusions': self._generate_conclusions(run_data)
            }
            
            # Render template
            template = self.env.get_template('report_template.html')
            html_content = template.render(**context)
            
            # Save report
            report_path = f"reports/report_{run_id}.html"
            os.makedirs("reports", exist_ok=True)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"Single run report generated: {report_path}")
            return html_content
        
        except Exception as e:
            logger.error(f"Failed to generate single run report: {e}")
            return None
    
    def generate_multi_run_report(self, run_ids: List[str]) -> Optional[str]:
        """
        Generate HTML comparison report for multiple runs.
        
        Args:
            run_ids: List of run IDs to compare
            
        Returns:
            HTML report string or None if failed
        """
        try:
            # Load data for all runs
            runs_data = []
            for run_id in run_ids:
                run_data = self._load_run_data(run_id)
                if run_data:
                    runs_data.append(run_data)
            
            if not runs_data:
                logger.error("No valid run data found for comparison")
                return None
            
            # Generate comparison content
            comparison_content = self._generate_multi_run_comparison(runs_data)
            
            # Generate context
            context = {
                'title': 'Multi-Run Comparison Report',
                'subtitle': f'Comparing {len(runs_data)} training runs',
                'is_comparison': True,
                'primary_color': self.config['primary_color'],
                'secondary_color': self.config['secondary_color'],
                'logo_url': self.config.get('logo_url', ''),
                'footer_text': self.config['footer_text'],
                'generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'git_commit': 'multiple',
                
                # Page numbers
                'executive_summary_page': '3',
                'dataset_analysis_page': '4',
                'comparison_page': '5',
                'error_analysis_page': '6',
                'conclusions_page': '7',
                
                # Content sections
                'executive_summary': self._generate_multi_run_executive_summary(runs_data),
                'dataset_analysis': self._generate_multi_run_dataset_analysis(runs_data),
                'comparison_content': comparison_content,
                'error_analysis': self._generate_multi_run_error_analysis(runs_data),
                'conclusions': self._generate_multi_run_conclusions(runs_data)
            }
            
            # Render template
            template = self.env.get_template('report_template.html')
            html_content = template.render(**context)
            
            # Save report
            report_filename = f"comparison_report_{'_'.join(run_ids[:3])}.html"
            report_path = f"reports/{report_filename}"
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"Multi-run comparison report generated: {report_path}")
            return html_content
        
        except Exception as e:
            logger.error(f"Failed to generate multi-run report: {e}")
            return None
    
    def generate_pdf_from_html(self, html_content: str) -> Optional[bytes]:
        """
        Generate PDF from HTML content using WeasyPrint.
        
        Args:
            html_content: HTML content string
            
        Returns:
            PDF content as bytes or None if failed
        """
        try:
            import weasyprint
            
            # Create temporary HTML file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as f:
                f.write(html_content)
                temp_html_path = f.name
            
            try:
                # Generate PDF
                pdf_content = weasyprint.HTML(filename=temp_html_path).write_pdf()
                return pdf_content
            
            finally:
                # Cleanup
                os.unlink(temp_html_path)
        
        except ImportError:
            logger.error("WeasyPrint not available. Install with: pip install weasyprint")
            return None
        except Exception as e:
            logger.error(f"Failed to generate PDF: {e}")
            return None
    
    def _load_run_data(self, run_id: str) -> Optional[Dict[str, Any]]:
        """Load all data for a training run."""
        try:
            run_dir = Path(f"checkpoints/{run_id}")
            
            if not run_dir.exists():
                logger.error(f"Run directory not found: {run_dir}")
                return None
            
            # Load configuration
            config_path = run_dir / "run_config.json"
            with open(config_path) as f:
                config = json.load(f)
            
            # Load training history
            history_path = run_dir / "history.pkl"
            if history_path.exists():
                with open(history_path, 'rb') as f:
                    history_data = pickle.load(f)
            else:
                history_data = {}
            
            # Load results
            results_path = run_dir / "results.json"
            if results_path.exists():
                with open(results_path) as f:
                    results = json.load(f)
            else:
                results = {}
            
            return {
                'run_id': run_id,
                'config': config,
                'history': history_data,
                'results': results
            }
        
        except Exception as e:
            logger.error(f"Failed to load run data for {run_id}: {e}")
            return None
    
    def _generate_executive_summary(self, run_data: Dict[str, Any]) -> str:
        """Generate executive summary for single run."""
        config = run_data['config']
        results = run_data.get('results', {})
        test_results = results.get('test_results', {})
        
        # Extract key metrics
        model_type = config.get('model', 'Unknown')
        dataset_name = config.get('dataset', 'Unknown')
        accuracy = test_results.get('accuracy', 0) * 100
        f1_score = test_results.get('f1_score', 0)
        epochs = config.get('epochs', 0)
        
        # Generate summary text with positive/negative phrasing
        if accuracy >= 90:
            accuracy_phrase = f"<span class='positive'>achieved excellent {accuracy:.1f}% test accuracy</span>"
        elif accuracy >= 80:
            accuracy_phrase = f"<span class='positive'>achieved good {accuracy:.1f}% test accuracy</span>"
        elif accuracy >= 70:
            accuracy_phrase = f"<span class='neutral'>achieved moderate {accuracy:.1f}% test accuracy</span>"
        else:
            accuracy_phrase = f"<span class='negative'>achieved limited {accuracy:.1f}% test accuracy</span>"
        
        summary = f"""
        <h2>Executive Summary</h2>
        <p>
            <strong>{config.get('run_label', run_data['run_id'])}</strong> 
            ({model_type} on {dataset_name}) trained for {epochs} epochs and 
            {accuracy_phrase} with an F1 score of {f1_score:.3f}.
        </p>
        """
        
        # Add model size info if available
        if 'model_params' in results:
            params = results['model_params']
            if params > 1000000:
                size_info = f"The model contains {params/1000000:.1f}M parameters"
            else:
                size_info = f"The model contains {params:,} parameters"
            
            summary += f"<p>{size_info}, making it suitable for {'edge deployment' if params < 1000000 else 'server deployment'}.</p>"
        
        # Add training time info
        if 'training_time' in results:
            training_time = results['training_time']
            if training_time < 60:
                time_info = f"Training completed in {training_time:.1f} seconds"
            elif training_time < 3600:
                time_info = f"Training completed in {training_time/60:.1f} minutes"
            else:
                time_info = f"Training completed in {training_time/3600:.1f} hours"
            
            summary += f"<p>{time_info}.</p>"
        
        return summary
    
    def _generate_dataset_analysis(self, run_data: Dict[str, Any]) -> str:
        """Generate dataset analysis section."""
        config = run_data['config']
        results = run_data.get('results', {})
        dataset_info = results.get('dataset_info', {})
        
        content = "<h2>Dataset Overview</h2>"
        
        if dataset_info:
            content += f"""
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="metric-value">{dataset_info.get('train_samples', 0):,}</div>
                    <div class="metric-label">Training Samples</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{dataset_info.get('val_samples', 0):,}</div>
                    <div class="metric-label">Validation Samples</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{dataset_info.get('test_samples', 0):,}</div>
                    <div class="metric-label">Test Samples</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{dataset_info.get('num_classes', 0)}</div>
                    <div class="metric-label">Classes</div>
                </div>
            </div>
            """
            
            # Add input shape info
            input_shape = dataset_info.get('input_shape', [])
            if input_shape:
                content += f"<p><strong>Input Shape:</strong> {input_shape}</p>"
        
        return content
    
    def _generate_model_architecture(self, run_data: Dict[str, Any]) -> str:
        """Generate model architecture section."""
        config = run_data['config']
        results = run_data.get('results', {})
        
        content = "<h2>Model Configuration</h2>"
        
        # Model type and parameters
        content += f"""
        <table class="comparison-table">
            <tr>
                <th>Property</th>
                <th>Value</th>
            </tr>
            <tr>
                <td>Model Type</td>
                <td>{config.get('model', 'Unknown')}</td>
            </tr>
            <tr>
                <td>Total Parameters</td>
                <td>{results.get('model_params', 0):,}</td>
            </tr>
            <tr>
                <td>Learning Rate</td>
                <td>{config.get('learning_rate', 0)}</td>
            </tr>
            <tr>
                <td>Batch Size</td>
                <td>{config.get('batch_size', 0)}</td>
            </tr>
            <tr>
                <td>Epochs</td>
                <td>{config.get('epochs', 0)}</td>
            </tr>
        </table>
        """
        
        return content
    
    def _generate_training_results(self, run_data: Dict[str, Any]) -> str:
        """Generate training results with plots."""
        history = run_data.get('history', {})
        
        content = "<h2>Training Progress</h2>"
        
        if 'loss' in history:
            # Create training curves plot
            fig = make_subplots(
                rows=1, cols=2,
                subplot_titles=('Loss', 'Accuracy'),
                specs=[[{"secondary_y": False}, {"secondary_y": False}]]
            )
            
            # Loss plot
            epochs = list(range(1, len(history['loss']) + 1))
            fig.add_trace(
                go.Scatter(x=epochs, y=history['loss'], name='Training Loss', line=dict(color='blue')),
                row=1, col=1
            )
            
            if 'val_loss' in history:
                fig.add_trace(
                    go.Scatter(x=epochs, y=history['val_loss'], name='Validation Loss', line=dict(color='red')),
                    row=1, col=1
                )
            
            # Accuracy plot
            if 'accuracy' in history:
                fig.add_trace(
                    go.Scatter(x=epochs, y=history['accuracy'], name='Training Accuracy', line=dict(color='green')),
                    row=1, col=2
                )
            
            if 'val_accuracy' in history:
                fig.add_trace(
                    go.Scatter(x=epochs, y=history['val_accuracy'], name='Validation Accuracy', line=dict(color='orange')),
                    row=1, col=2
                )
            
            fig.update_layout(
                title='Training Progress',
                showlegend=True,
                height=400
            )
            
            # Convert to HTML
            plot_html = pio.to_html(fig, include_plotlyjs=False, div_id="training-curves")
            content += f'<div class="chart-container">{plot_html}</div>'
        
        return content
    
    def _generate_evaluation_metrics(self, run_data: Dict[str, Any]) -> str:
        """Generate evaluation metrics section."""
        results = run_data.get('results', {})
        test_results = results.get('test_results', {})
        
        content = "<h2>Test Performance</h2>"
        
        # Key metrics
        content += f"""
        <div class="metrics-grid">
            <div class="metric-card">
                <div class="metric-value">{test_results.get('accuracy', 0)*100:.1f}%</div>
                <div class="metric-label">Test Accuracy</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{test_results.get('loss', 0):.3f}</div>
                <div class="metric-label">Test Loss</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{test_results.get('f1_score', 0):.3f}</div>
                <div class="metric-label">F1 Score</div>
            </div>
        </div>
        """
        
        # Confusion matrix
        if 'confusion_matrix' in results:
            cm = np.array(results['confusion_matrix'])
            
            # Create confusion matrix heatmap
            fig = px.imshow(
                cm,
                text_auto=True,
                aspect="auto",
                title="Confusion Matrix",
                color_continuous_scale="Blues"
            )
            
            plot_html = pio.to_html(fig, include_plotlyjs=False, div_id="confusion-matrix")
            content += f'<div class="chart-container">{plot_html}</div>'
        
        return content
    
    def _generate_error_analysis(self, run_data: Dict[str, Any]) -> str:
        """Generate error analysis section."""
        content = "<h2>Error Analysis</h2>"
        
        content += """
        <div class="error-analysis">
            <h3>Misclassification Patterns</h3>
            <p>Detailed error analysis would appear here with specific failure cases and patterns.</p>
        </div>
        """
        
        return content
    
    def _generate_conclusions(self, run_data: Dict[str, Any]) -> str:
        """Generate conclusions section."""
        test_results = run_data.get('results', {}).get('test_results', {})
        accuracy = test_results.get('accuracy', 0) * 100
        
        conclusions = "<h3>Key Findings</h3><ul>"
        
        if accuracy >= 90:
            conclusions += f"<li>The model achieved excellent performance with {accuracy:.1f}% accuracy, indicating strong generalization.</li>"
            conclusions += "<li>The current architecture is well-suited for the task and ready for deployment.</li>"
        elif accuracy >= 80:
            conclusions += f"<li>The model achieved good performance with {accuracy:.1f}% accuracy.</li>"
            conclusions += "<li>Minor improvements could be achieved through hyperparameter tuning or data augmentation.</li>"
        else:
            conclusions += f"<li>The model achieved {accuracy:.1f}% accuracy, indicating room for improvement.</li>"
            conclusions += "<li>Consider alternative architectures, additional training data, or feature engineering.</li>"
        
        conclusions += "</ul><h3>Next Steps</h3><ul>"
        conclusions += "<li>Evaluate model performance on additional test data.</li>"
        conclusions += "<li>Consider model optimization for deployment constraints.</li>"
        conclusions += "<li>Implement monitoring for production deployment.</li>"
        conclusions += "</ul>"
        
        return conclusions
    
    def _generate_multi_run_comparison(self, runs_data: List[Dict[str, Any]]) -> str:
        """Generate multi-run comparison content."""
        content = "<h2>Performance Comparison</h2>"
        
        # Create comparison table
        comparison_data = []
        for run_data in runs_data:
            config = run_data['config']
            test_results = run_data.get('results', {}).get('test_results', {})
            
            row = {
                'Run ID': run_data['run_id'],
                'Model': config.get('model', 'Unknown'),
                'Accuracy': f"{test_results.get('accuracy', 0)*100:.1f}%",
                'F1 Score': f"{test_results.get('f1_score', 0):.3f}",
                'Loss': f"{test_results.get('loss', 0):.3f}",
                'Parameters': f"{run_data.get('results', {}).get('model_params', 0):,}"
            }
            comparison_data.append(row)
        
        # Generate HTML table
        content += '<table class="comparison-table"><tr>'
        for header in comparison_data[0].keys():
            content += f'<th>{header}</th>'
        content += '</tr>'
        
        for row in comparison_data:
            content += '<tr>'
            for value in row.values():
                content += f'<td>{value}</td>'
            content += '</tr>'
        content += '</table>'
        
        # Add comparison plots
        if len(runs_data) > 1:
            # Create side-by-side accuracy comparison
            run_labels = [run['config'].get('run_label', run['run_id']) for run in runs_data]
            accuracies = [run.get('results', {}).get('test_results', {}).get('accuracy', 0)*100 
                         for run in runs_data]
            
            fig = px.bar(
                x=run_labels,
                y=accuracies,
                title="Test Accuracy Comparison",
                labels={'x': 'Run', 'y': 'Accuracy (%)'}
            )
            
            plot_html = pio.to_html(fig, include_plotlyjs=False, div_id="accuracy-comparison")
            content += f'<div class="chart-container">{plot_html}</div>'
        
        return content
    
    def _generate_multi_run_executive_summary(self, runs_data: List[Dict[str, Any]]) -> str:
        """Generate executive summary for multi-run comparison."""
        if not runs_data:
            return "<p>No runs available for comparison.</p>"
        
        # Find best performing run
        best_run = max(runs_data, 
                      key=lambda x: x.get('results', {}).get('test_results', {}).get('accuracy', 0))
        
        best_accuracy = best_run.get('results', {}).get('test_results', {}).get('accuracy', 0) * 100
        best_model = best_run['config'].get('model', 'Unknown')
        best_label = best_run['config'].get('run_label', best_run['run_id'])
        
        summary = f"""
        <h2>Multi-Run Comparison Summary</h2>
        <p>
            Compared {len(runs_data)} training runs across different model architectures and configurations.
            <span class='highlight'>{best_label} ({best_model})</span> achieved the highest performance 
            with <span class='positive'>{best_accuracy:.1f}% test accuracy</span>.
        </p>
        """
        
        # Add performance ranking
        sorted_runs = sorted(runs_data, 
                           key=lambda x: x.get('results', {}).get('test_results', {}).get('accuracy', 0),
                           reverse=True)
        
        summary += "<p><strong>Performance Ranking:</strong></p><ol>"
        for i, run in enumerate(sorted_runs[:3]):  # Top 3
            acc = run.get('results', {}).get('test_results', {}).get('accuracy', 0) * 100
            model = run['config'].get('model', 'Unknown')
            label = run['config'].get('run_label', run['run_id'])
            summary += f"<li>{label} ({model}): {acc:.1f}%</li>"
        summary += "</ol>"
        
        return summary
    
    def _generate_multi_run_dataset_analysis(self, runs_data: List[Dict[str, Any]]) -> str:
        """Generate dataset analysis for multi-run comparison."""
        # Use dataset info from first run (assuming same dataset)
        if runs_data:
            return self._generate_dataset_analysis(runs_data[0])
        return "<p>No dataset information available.</p>"
    
    def _generate_multi_run_error_analysis(self, runs_data: List[Dict[str, Any]]) -> str:
        """Generate error analysis for multi-run comparison."""
        content = "<h2>Comparative Error Analysis</h2>"
        
        content += """
        <div class="error-analysis">
            <h3>Cross-Model Error Patterns</h3>
            <p>Comparative analysis of error patterns across different model architectures would appear here.</p>
        </div>
        """
        
        return content
    
    def _generate_multi_run_conclusions(self, runs_data: List[Dict[str, Any]]) -> str:
        """Generate conclusions for multi-run comparison."""
        if not runs_data:
            return "<p>No runs available for analysis.</p>"
        
        # Find best run
        best_run = max(runs_data, 
                      key=lambda x: x.get('results', {}).get('test_results', {}).get('accuracy', 0))
        
        best_model = best_run['config'].get('model', 'Unknown')
        best_label = best_run['config'].get('run_label', best_run['run_id'])
        
        conclusions = f"""
        <h3>Key Findings</h3>
        <ul>
            <li>The {best_model} architecture ({best_label}) demonstrated superior performance across evaluation metrics.</li>
            <li>Model architecture choice significantly impacts performance on this dataset.</li>
            <li>Training hyperparameters show consistent patterns across runs.</li>
        </ul>
        
        <h3>Recommendations</h3>
        <ul>
            <li>Deploy the {best_model} model for production use.</li>
            <li>Further optimize the best-performing architecture with extended hyperparameter search.</li>
            <li>Consider ensemble methods combining top-performing models.</li>
        </ul>
        """
        
        return conclusions
